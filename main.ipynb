{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "attribute_names = [\n",
    "    \"health\", \"strength\", \"dexterity\", \"perception\", \n",
    "    \"intelligence\", \"charisma\", \"stamina\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load risk-model\n",
    "risk_tokenizer = AutoTokenizer.from_pretrained(\"samwu1/risk-model\")\n",
    "risk_model = BertForSequenceClassification.from_pretrained(\"samwu1/risk-model\")\n",
    "\n",
    "def get_risk_output(input):\n",
    "    encoding = risk_tokenizer(input, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    risk_model.to(device)\n",
    "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "    with torch.no_grad():\n",
    "        output = risk_model(**encoding)\n",
    "    return output.logits[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load risk-attribute\n",
    "attribute_tokenizer =  AutoTokenizer.from_pretrained(\"samwu1/attribute-model\")\n",
    "attribute_model = BertForSequenceClassification.from_pretrained(\"samwu1/attribute-model\")\n",
    "\n",
    "def get_attribute_output(dm_prompt, user_input, dm_output):\n",
    "    input = f\"<DM>{dm_prompt}</DM>\\n<Player>{user_input}</Player>\\n<DM>{dm_output}</DM>\"\n",
    "    encoding = attribute_tokenizer(input, return_tensors=\"pt\")\n",
    "    encoding = {k: v.to(attribute_model.device) for k,v in encoding.items()}\n",
    "    prediction = attribute_model(**encoding).logits\n",
    "    return prediction.squeeze().tolist()\n",
    "\n",
    "def update_attributes(attributes, d_attributes):\n",
    "    new_attributes_stats = attributes + d_attributes\n",
    "    attributes_string = \"\\n\".join(f\"{name}:{value:.2f}\" for name, value in zip(attribute_names, new_attributes_stats))\n",
    "    return attributes_string, new_attributes_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "#load summarization pipeline (defaults to bart-large-cnn)\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "def get_summarizer_output(input):\n",
    "    num_tokens = len(summarizer.tokenizer.encode(input, truncation=False))\n",
    "    num_tokens = 20 if num_tokens<10 else num_tokens\n",
    "    summary = summarizer(input, max_length=int(num_tokens*.8), min_length=int(num_tokens*.3), do_sample=True)\n",
    "    return summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dm-model\n",
    "# Load tokenizer and model\n",
    "dm_tokenizer = AutoTokenizer.from_pretrained(\"samwu1/dm-model\")\n",
    "dm_model = AutoModelForCausalLM.from_pretrained(\"samwu1/dm-model\", torch_dtype=torch.float16)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dm_model.to(device)\n",
    "\n",
    "def get_dm_output(dm_input, user_input):\n",
    "    message = [{\"role\": \"system\",\"content\": dm_input,},{\"role\": \"user\", \"content\": user_input}]\n",
    "    prompt = dm_tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "    input_ids = dm_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(dm_model.device)\n",
    "    with torch.inference_mode():\n",
    "        generated_ids = dm_model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=80,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            temperature=.8,\n",
    "        )\n",
    "    generated_text = dm_tokenizer.decode(generated_ids[0], skip_special_tokens=False)\n",
    "    return generated_text.split(\"<|assistant|>\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init setup: generate random attributes, get inital scenario from the dataset as a introduction:\n",
    "attributes_stats = np.random.rand(7)\n",
    "attribute_string, attributes_stats = update_attributes(attributes_stats, 0)\n",
    "scenarios = pd.read_csv(\"data/scenario-dataset.csv\")[\"scenarios\"].to_list()\n",
    "intro_scene = random.choice(scenarios)\n",
    "dm_match = re.search(r\"<DM>(.*?)</DM>\", intro_scene)\n",
    "dm_prompt = dm_match.group(1) if dm_match else \"\"\n",
    "history = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health:0.79\n",
      "strength:0.98\n",
      "dexterity:0.43\n",
      "perception:0.69\n",
      "intelligence:0.79\n",
      "charisma:0.03\n",
      "stamina:0.40\n",
      "You stand in front of a locked wooden door, the faint sound of footsteps echoing beyond it. The door has no visible keyhole. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 16, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i try to climb through the window\n",
      "\n",
      "loading risk:\n",
      "Risk:0.750372052192688\n",
      "Rolling Dice:\n",
      "0.06847590078297172\n",
      "<Roll: Success>\n",
      "\n",
      "health:0.76\n",
      "strength:1.01\n",
      "dexterity:0.44\n",
      "perception:0.68\n",
      "intelligence:0.81\n",
      "charisma:0.03\n",
      "stamina:0.43\n",
      "\n",
      "You grip the smallest crevice between two stones and haul yourself up, heart pounding, with a swift, decisive movement. The door sways open to reveal a narrow stone hallway where whispers drift from deeper within. Pressing closer, you overhear two people arguing about a missing artifact and someone called \"The Warden,\" with urgency about recovering\n",
      "\n",
      "\n",
      "loading risk:\n",
      "Risk:0.7257801294326782\n",
      "Rolling Dice:\n",
      "0.7155067787283218\n",
      "<Roll: Success>\n",
      "\n",
      "health:0.75\n",
      "strength:1.02\n",
      "dexterity:0.48\n",
      "perception:0.69\n",
      "intelligence:0.87\n",
      "charisma:0.06\n",
      "stamina:0.44\n",
      "\n",
      "You stand your ground, your hand lifting in a defensive gesture as the conversation halts. You listen carefully, taking note of the key players and their motivations. You manage to gather crucial intelligence, enough to warn of The Warden's presence and potential actions. With that knowledge, you prepare to defend your findings and assert your right to intellectual property, earning respect and\n",
      "quit\n",
      "\n",
      "loading risk:\n",
      "Risk:0.5978377461433411\n",
      "Rolling Dice:\n",
      "0.016896301554056747\n",
      "<Roll: Success>\n",
      "\n",
      "health:0.80\n",
      "strength:0.98\n",
      "dexterity:0.50\n",
      "perception:0.80\n",
      "intelligence:0.99\n",
      "charisma:0.06\n",
      "stamina:0.35\n",
      "\n",
      "You stand your ground, your hand lifting in a defensive gesture as the conversation halts. You listen carefully, taking note of the key players and their motivations. You manage to gather crucial intelligence, enough to warn of The Warden's presence and potential actions. With that knowledge, you prepare to defend your findings and assert your right to intellectual property, earning respect and\n"
     ]
    }
   ],
   "source": [
    "# putting it all together \n",
    "import textwrap\n",
    "print(attribute_string, \"\\n\")\n",
    "print(\"DM: \", dm_prompt, \"\\n\")\n",
    "user_input = None\n",
    "while user_input!=\"quit\":\n",
    "    user_input = input()\n",
    "    print(f\"\\nPlayer: {user_input}\")\n",
    "    print(\"\\nloading risk:\")\n",
    "    risk_input = \"\\n\".join([attribute_string, dm_prompt, user_input])\n",
    "    #feed risk_input into risk_model\n",
    "    risk = get_risk_output(risk_input)\n",
    "    print(f\"Risk: {risk}\\n\")\n",
    "\n",
    "    #rolling dice\n",
    "    print(\"Rolling Dice:\")\n",
    "    dice_roll = np.random.rand()\n",
    "    print(dice_roll, \"\\n\")\n",
    "    roll = \"<Roll: Success>\" if dice_roll<risk else \"<Roll: Failure>\"\n",
    "    print(f\"{roll}\\n\")\n",
    "\n",
    "    #generate history:\n",
    "    prev_text = get_summarizer_output(history)\n",
    "    history = prev_text+\"\\n\"+dm_prompt+\"\\n\"+user_input\n",
    "\n",
    "    #feed history, attribute, dm_prompt, roll, and user_input to dm-model\n",
    "    dm_input = f\"history:{{{history}}}\\n\\n({attribute_string})\\n\\n<DM>{dm_prompt}</DM>\\n\\n{roll}\"\n",
    "    dm_output = get_dm_output(dm_input, user_input)\n",
    "\n",
    "    #update attributes\n",
    "    d_attributes = get_attribute_output(dm_prompt, user_input, dm_output)\n",
    "    attribute_string, attributes_stats = update_attributes(attributes_stats, d_attributes)\n",
    "\n",
    "    #reprint output\n",
    "    wrapped_dm_output = textwrap.wrap(dm_output, width=100)\n",
    "    print(\"DM:\", end=' ')\n",
    "    for line in wrapped_dm_output:\n",
    "        print(line)\n",
    "\n",
    "    print(\"\\n\"+attribute_string)\n",
    "\n",
    "    #update dm_prompt\n",
    "    dm_prompt = dm_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
